{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fa6220b-f48c-414c-a005-7998db1dcd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/02/13 15:00:27 WARN Utils: Your hostname, GOUTAM, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "26/02/13 15:00:27 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/13 15:00:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Question No 10-Book Metadata Extraction and Analysis\n",
    "# create spark session \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Book Metadata Analysis\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d0d15c4-80e2-4510-ac28-cb8351ce96eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 15:00:35 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: /home/goutam/CSL7110/pyspark-tutorial/data/*.txt.\n",
      "java.io.FileNotFoundException: File /home/goutam/CSL7110/pyspark-tutorial/data/*.txt does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:980)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1301)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:970)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.sinks.FileStreamSink$.hasMetadata(FileStreamSink.scala:58)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:384)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:343)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:339)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:339)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:289)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:114)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:112)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:108)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:57)\n",
      "\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:535)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.text(DataFrameReader.scala:328)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+\n",
      "|text                                          |\n",
      "+----------------------------------------------+\n",
      "|----- BOOK -----                              |\n",
      "|Title: The Extermination of the American Bison|\n",
      "|Author: William T. Hornaday                   |\n",
      "|Link: http://www.gutenberg.org/ebooks/17748   |\n",
      "|Bookshelf: Animal                             |\n",
      "+----------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "#Load Project Gutenberg Dataset\n",
    "#Load all text files:\n",
    "books_df = spark.read.text(\"/home/goutam/CSL7110/pyspark-tutorial/data/*.txt\")\n",
    "\n",
    "books_df = books_df.withColumnRenamed(\"value\", \"text\")\n",
    "\n",
    "books_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fab4cc4c-1b08-433d-a890-b1a4d2cef743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add filename column\n",
    "books_df = books_df.withColumn(\n",
    "    \"file_name\",\n",
    "    input_file_name()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9692c91a-7a01-4a6d-9cf8-2880dafbdb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reorder columns:\n",
    "books_df = books_df.select(\"file_name\", \"text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98fa1c48-19a9-4b9c-8843-0f60d44a8dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metadata Extraction (Regex)\n",
    "# Extract File \n",
    "books_df = books_df.withColumn(\n",
    "    \"title\",\n",
    "    regexp_extract(\n",
    "        col(\"text\"),\n",
    "        r\"Title:\\s*(.*)\",\n",
    "        1\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05bf62a3-edcd-471b-9946-e9a5d4eb302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Release Date\n",
    "books_df = books_df.withColumn(\n",
    "    \"release_date\",\n",
    "    regexp_extract(\n",
    "        col(\"text\"),\n",
    "        r\"Release Date:\\s*(.*)\",\n",
    "        1\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f527e2f-c3a1-45dd-8dc6-91fa70d2d898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Language\n",
    "books_df = books_df.withColumn(\n",
    "    \"language\",\n",
    "    regexp_extract(\n",
    "        col(\"text\"),\n",
    "        r\"Language:\\s*(.*)\",\n",
    "        1\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5306d5bd-558a-4d51-9187-712464d94f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Encoding\n",
    "books_df = books_df.withColumn(\n",
    "    \"encoding\",\n",
    "    regexp_extract(\n",
    "        col(\"text\"),\n",
    "        r\"Character set encoding:\\s*(.*)\",\n",
    "        1\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78ae1df7-e5bd-4371-bc22-282a05ae0277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Metadata (Important Step)\n",
    "metadata_df = books_df.filter(col(\"title\") != \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f877b1a-24d3-45a7-b375-80ddac685e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+--------------------------------------------------------------------------------+------------+--------+--------+\n",
      "|file_name                                                               |title                                                                           |release_date|language|encoding|\n",
      "+------------------------------------------------------------------------+--------------------------------------------------------------------------------+------------+--------+--------+\n",
      "|file:///home/goutam/CSL7110/pyspark-tutorial/data/gutenberg_metadata.txt|The Extermination of the American Bison                                         |            |        |        |\n",
      "|file:///home/goutam/CSL7110/pyspark-tutorial/data/gutenberg_metadata.txt|Deadfalls and Snares                                                            |            |        |        |\n",
      "|file:///home/goutam/CSL7110/pyspark-tutorial/data/gutenberg_metadata.txt|Artistic Anatomy of Animals                                                     |            |        |        |\n",
      "|file:///home/goutam/CSL7110/pyspark-tutorial/data/gutenberg_metadata.txt|Birds, Illustrated                                                              |            |        |        |\n",
      "|file:///home/goutam/CSL7110/pyspark-tutorial/data/gutenberg_metadata.txt|On Snake-Poison: Its Action and Its Antidote                                    |            |        |        |\n",
      "|file:///home/goutam/CSL7110/pyspark-tutorial/data/gutenberg_metadata.txt|Fifty Years a Hunter and Trapper                                                |            |        |        |\n",
      "|file:///home/goutam/CSL7110/pyspark-tutorial/data/gutenberg_metadata.txt|What Bird is That?                                                              |            |        |        |\n",
      "|file:///home/goutam/CSL7110/pyspark-tutorial/data/gutenberg_metadata.txt|Fox Trapping: A Book of Instruction Telling How to Trap, Snare, Poison and Shoot|            |        |        |\n",
      "|file:///home/goutam/CSL7110/pyspark-tutorial/data/gutenberg_metadata.txt|A Guide for the Study of Animals                                                |            |        |        |\n",
      "|file:///home/goutam/CSL7110/pyspark-tutorial/data/gutenberg_metadata.txt|Our Vanishing Wild Life: Its Extermination and Preservation                     |            |        |        |\n",
      "|file:///home/goutam/CSL7110/pyspark-tutorial/data/gutenberg_metadata.txt|Wolf and Coyote Trapping: An Up-to-Date Wolf Hunter's Guide                     |            |        |        |\n",
      "|file:///home/goutam/CSL7110/pyspark-tutorial/data/gutenberg_metadata.txt|Birds and Man                                                                   |            |        |        |\n",
      "|file:///home/goutam/CSL7110/pyspark-tutorial/data/gutenberg_metadata.txt|Zoological Mythology; or, The Legends of Animals, Volume 1 (of 2)               |            |        |        |\n",
      "|file:///home/goutam/CSL7110/pyspark-tutorial/data/gutenberg_metadata.txt|Antarctic Penguins: A Study of Their Social Habits                              |            |        |        |\n",
      "|file:///home/goutam/CSL7110/pyspark-tutorial/data/gutenberg_metadata.txt|Butterflies and Moths (British)                                                 |            |        |        |\n",
      "|file:///home/goutam/CSL7110/pyspark-tutorial/data/gutenberg_metadata.txt|The Bird Book                                                                   |            |        |        |\n",
      "|file:///home/goutam/CSL7110/pyspark-tutorial/data/gutenberg_metadata.txt|Hunting in Many Lands: The Book of the Boone and Crockett Club                  |            |        |        |\n",
      "|file:///home/goutam/CSL7110/pyspark-tutorial/data/gutenberg_metadata.txt|Wild Animals at Home                                                            |            |        |        |\n",
      "|file:///home/goutam/CSL7110/pyspark-tutorial/data/gutenberg_metadata.txt|Birds from North Borneo                                                         |            |        |        |\n",
      "|file:///home/goutam/CSL7110/pyspark-tutorial/data/gutenberg_metadata.txt|Sketches of the Natural History of Ceylon                                       |            |        |        |\n",
      "+------------------------------------------------------------------------+--------------------------------------------------------------------------------+------------+--------+--------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# show result\n",
    "metadata_df.select(\n",
    "    \"file_name\",\n",
    "    \"title\",\n",
    "    \"release_date\",\n",
    "    \"language\",\n",
    "    \"encoding\"\n",
    ").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "951d205c-283d-4ae9-9129-6f4067948e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5 â€” ANALYSIS PART\n",
    "#Books Released Per Year\n",
    "metadata_df = metadata_df.withColumn(\n",
    "    \"year\",\n",
    "    regexp_extract(col(\"release_date\"), r\"(\\d{4})\", 1)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8f0d7a7-5b96-41b2-8dcd-6a9dd6f6594d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|year|count|\n",
      "+----+-----+\n",
      "|    |15331|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count books\n",
    "books_per_year = metadata_df.groupBy(\"year\").count()\n",
    "books_per_year.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d305f63f-c748-4100-b304-261029807a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|language|count|\n",
      "+--------+-----+\n",
      "|        |15329|\n",
      "+--------+-----+\n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "#ðŸ”¹ 2. Most Common Language\n",
    "metadata_df.groupBy(\"language\") \\\n",
    "    .count() \\\n",
    "    .orderBy(desc(\"count\")) \\\n",
    "    .show(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e723581f-35fa-47e7-9e25-ac7ad91e3f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|avg(title_length)|\n",
      "+-----------------+\n",
      "|35.67118909399257|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ðŸ”¹ 3. Average Title Length\n",
    "\n",
    "metadata_df = metadata_df.withColumn(\n",
    "    \"title_length\",\n",
    "    length(col(\"title\"))\n",
    ")\n",
    "\n",
    "metadata_df.select(\n",
    "    avg(\"title_length\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7420bca1-4c55-486c-902a-58890a030e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTION 11 â€” TF-IDF and Book Similarity\n",
    "#Step 1 â€” Text Preprocessing\n",
    "# Remove Gutenberg Header/Footer\n",
    "clean_df = books_df.withColumn(\n",
    "    \"clean_text\",\n",
    "    regexp_replace(col(\"text\"),\n",
    "    \"\\\\*\\\\*\\\\*.*?\\\\*\\\\*\\\\*\", \"\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3f6b65a-a2b6-4011-b678-9cbeb9970e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lowercase + Remove Punctuation\n",
    "clean_df = clean_df.withColumn(\n",
    "    \"clean_text\",\n",
    "    lower(regexp_replace(col(\"clean_text\"), \"[^a-zA-Z\\\\s]\", \" \"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9bb66807-d40f-4fa0-8144-63a8ef7cf5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"clean_text\", outputCol=\"words\")\n",
    "words_df = tokenizer.transform(clean_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e7da0b07-b63d-4087-a7e2-13ff8252b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Stopwords\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "filtered_df = remover.transform(words_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c67b5a70-ab83-44d4-95f1-070a9211277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2 â€” TF-IDF Calculation\n",
    "#Term Frequency\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "tf = HashingTF(inputCol=\"filtered\", outputCol=\"tf_features\")\n",
    "tf_df = tf.transform(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3cca1c28-ba8a-4531-a2b0-fdae6c494c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#IDF\n",
    "idf = IDF(inputCol=\"tf_features\", outputCol=\"tfidf_features\")\n",
    "idf_model = idf.fit(tf_df)\n",
    "\n",
    "tfidf_df = idf_model.transform(tf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a586b70-e2d2-40dc-8120-d568b6bcdfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3 â€” Cosine Similarity\n",
    "#Convert to Vectors\n",
    "from pyspark.ml.linalg import Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "23280974-2a32-4d83-9a42-8d60d47a6e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Self Join for Pairwise Similarity\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import FloatType\n",
    "import numpy as np\n",
    "\n",
    "def cosine_sim(v1, v2):\n",
    "    return float(\n",
    "        v1.dot(v2) /\n",
    "        (np.linalg.norm(v1.toArray()) *\n",
    "         np.linalg.norm(v2.toArray()))\n",
    "    )\n",
    "\n",
    "cosine_udf = udf(cosine_sim, FloatType())\n",
    "\n",
    "pairs = tfidf_df.alias(\"a\").crossJoin(\n",
    "    tfidf_df.alias(\"b\")\n",
    ").filter(col(\"a.file_name\") != col(\"b.file_name\"))\n",
    "\n",
    "similarity_df = pairs.withColumn(\n",
    "    \"similarity\",\n",
    "    cosine_udf(\n",
    "        col(\"a.tfidf_features\"),\n",
    "        col(\"b.tfidf_features\")\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a1670ca1-dc26-49ea-ad04-dde0ce03a59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|file_name|similarity|\n",
      "+---------+----------+\n",
      "+---------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/13 15:31:31 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n"
     ]
    }
   ],
   "source": [
    "#Top 5 Similar Books to \"10.txt\"\n",
    "similarity_df.filter(col(\"a.file_name\")==\"10.txt\") \\\n",
    ".orderBy(desc(\"similarity\")) \\\n",
    ".select(\"b.file_name\",\"similarity\") \\\n",
    ".show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6dcb154a-f32e-40b6-8694-3d36a93fd2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTION 12 â€” Author Influence Network\n",
    "#Step 1 â€” Extract Author & Year\n",
    "from pyspark.sql.functions import col, regexp_extract, when\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "author_df = metadata_df.withColumn(\n",
    "    \"author\",\n",
    "    regexp_extract(col(\"text\"), \"(?i)author:\\\\s*(.*)\", 1)\n",
    ").withColumn(\n",
    "    \"year_raw\",\n",
    "    regexp_extract(col(\"release_date\"), r\"(\\\\d{4})\", 1)\n",
    ").withColumn(\n",
    "    \"year\",\n",
    "    when(col(\"year_raw\") == \"\", None)\n",
    "    .otherwise(col(\"year_raw\").cast(IntegerType()))\n",
    ").drop(\"year_raw\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d5f405ea-5dc9-4e91-a0ce-943b51580f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2 â€” Build Influence Edges\n",
    "from pyspark.sql.functions import col, abs\n",
    "\n",
    "X = 5\n",
    "\n",
    "a = author_df.alias(\"a\")\n",
    "b = author_df.alias(\"b\")\n",
    "\n",
    "edges = a.join(\n",
    "    b,\n",
    "    col(\"a.author\") != col(\"b.author\")\n",
    ").filter(\n",
    "    abs(col(\"a.year\") - col(\"b.year\")) <= X\n",
    ").select(\n",
    "    col(\"a.author\").alias(\"author1\"),\n",
    "    col(\"b.author\").alias(\"author2\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eb7aa111-521d-4af6-bd84-89cb8d43fb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3 â€” Degree Analysis\n",
    "#In-Degree\n",
    "in_degree = edges.groupBy(\"author2\") \\\n",
    ".count() \\\n",
    ".withColumnRenamed(\"count\",\"in_degree\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dbe14341-440f-4f00-b082-609770e06c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Out-Degree\n",
    "out_degree = edges.groupBy(\"author1\") \\\n",
    ".count() \\\n",
    ".withColumnRenamed(\"count\",\"out_degree\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d7a1a22d-f46d-481a-b66e-27eaed7834d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|author2|in_degree|\n",
      "+-------+---------+\n",
      "+-------+---------+\n",
      "\n",
      "+-------+----------+\n",
      "|author1|out_degree|\n",
      "+-------+----------+\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Top Authors\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "in_degree.orderBy(desc(\"in_degree\")).show(5)\n",
    "out_degree.orderBy(desc(\"out_degree\")).show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffd5a5d-8793-4241-95f2-a860b3f63d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
